Здесь собраны базовые decoder-only трансформеры, реализованные на Python с использованием библиотеки PyTorch (наследование от nn.Module).

Цель данного проекта — не наибольшая алгоритмическая эффективность, но отработка и закрепление знаний c помощью аккуратной реализации основных архитектурных особенностей моделей.

**Список  моделей:**
- Gemma
- GPT 1, 2
- Llama
- Mistral
- Mixtral

**Были реализованы:**
- Обучаемое и роторное позиционное кодирование (RoPE)
- Механизм внимания и его расширения: Multi Head, Grouped Query и Multi Query.
- Методы повышения вычислительной эффективности внимания: KV-cache, sliding window attention
- Слой Sparse Mixture of Experts
- RMS нормализация
- Слои активации: GELU, SwiGLU, GeGLU
- Возможность контроля температуры, применения top-k и top-p стратегий при генерации токенов
